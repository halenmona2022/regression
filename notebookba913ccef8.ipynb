{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halenmona2022/regression/blob/main/notebookba913ccef8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fairseq 模块\n",
        "\n",
        "fairseq模块是一个基于PyTorch的开源序列建模工具库，用于研究和开发自然语言处理任务中的序列建模算法。fairseq库提供了一些常用的序列建模模型和训练/推理工具，例如transformer、LSTM、单向/双向RNN等模型，以及beam search、sampling等推理方法。fairseq还提供了高效的分布式训练模块，支持多GPU集群和多机器分布式训练，能够帮助科研人员在大规模数据上进行高质量的深度学习研究\n",
        "\n",
        "fairseq模块包括以下几种组件：\n",
        "\n",
        "* 数据预处理：提供数据处理的API，帮助用户从原始语言数据构建词典、分割/编码输入序列等；\n",
        "* 训练管道：支持单机训练、多GPU训练和多机器分布式训练。训练过程中自动进行梯度裁剪、参数更新、优化器等操作；\n",
        "* 推理/评估管道：支持beam search、sampling等推理方法，在测试集上评估最终的模型效果；\n",
        "* 模型和组件：提供若干常见的序列建模模型，例如transformer、LSTM、单向/双向RNN等，同时也支持自定义模型和组件的开发；\n",
        "* 优化器：实现了常用的优化器，例如Adam、Adagrad、RMSprop等\n"
      ],
      "metadata": {
        "id": "JlVd-5g8wG6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装fairseq\n",
        "!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "!pip install --upgrade jupyter ipywidgets\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "!cd fairseq && git checkout 9a1c497\n",
        "!pip install --upgrade ./fairseq/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T17:24:34.578849Z",
          "iopub.execute_input": "2023-05-18T17:24:34.579211Z",
          "iopub.status.idle": "2023-05-18T17:26:24.475128Z",
          "shell.execute_reply.started": "2023-05-18T17:24:34.579166Z",
          "shell.execute_reply": "2023-05-18T17:26:24.473678Z"
        },
        "trusted": true,
        "id": "PnX1KJt1wG6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----- Environment 环境准备-----\n",
        "\n",
        "\n",
        "\n",
        "# import python\n",
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "# import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# import visualization\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install fairseq\n",
        "from fairseq import utils\n",
        "\n",
        "# Cuda environment\n",
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# set random seed\n",
        "seed = 114514\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-18T17:27:23.129723Z",
          "iopub.execute_input": "2023-05-18T17:27:23.130391Z",
          "iopub.status.idle": "2023-05-18T17:27:36.335692Z",
          "shell.execute_reply.started": "2023-05-18T17:27:23.130339Z",
          "shell.execute_reply": "2023-05-18T17:27:36.334178Z"
        },
        "trusted": true,
        "id": "i9qdMJUWwG6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载资料集\n",
        "data_dir = '/kaggle/working/DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "    \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ml2023.hw5.data.tgz\",\n",
        "    \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ml2023.hw5.test.tgz\"\n",
        ")\n",
        "file_names = (\n",
        "    'ted2020.tgz', # train & dev\n",
        "    'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = prefix/f\n",
        "    if not path.exists():\n",
        "        !wget {u} -O {path}\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n",
        "!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}\n",
        "\n",
        "# 设置语言\n",
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'\n",
        "\n",
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T17:28:37.693007Z",
          "iopub.execute_input": "2023-05-18T17:28:37.693422Z",
          "iopub.status.idle": "2023-05-18T17:28:46.005337Z",
          "shell.execute_reply.started": "2023-05-18T17:28:37.693386Z",
          "shell.execute_reply": "2023-05-18T17:28:46.003645Z"
        },
        "trusted": true,
        "id": "bfe1687mwG6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 档案前处理\n",
        "import re\n",
        "\n",
        "# strQ2B: 将一个字符串中的全角字符转成半角字符，用于处理中文字符串\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"全角字符 -> 半角ASCII\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # 全宽空格直接换成空格\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全宽的别的字符直接转了\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "#清洗和整理一个句子，包括去除括号中的文本、去除特殊字符、将全角字符转成半角字符、在标点符号前后添加空格\n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\", \"\", s) # remove ([text]) 去掉括号内的语句\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "#求句子长度，可以用来限制句子最大最小查高度\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "## 处理整个语料库，按照指定的方式进行清洗和处理，并移除不符合要求的句子，保存处理后的结果到新的文件中\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    # ratio：句子长度比例的阈值，用于过滤长度差异太大的句子，默认为9。\n",
        "    # 例如，如果ratio为9，则表示源语言和目标语言长度之比不能超过9或者小于1/9，否则句子将被过滤掉。\n",
        "    # max_len：句子的最大长度，用于过滤长度超过指定值的句子，默认为1000。\n",
        "    # min_len：句子的最小长度，用于过滤长度小于指定值的句子，默认为1。\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)\n",
        "\n"
      ],
      "metadata": {
        "id": "ujXrJtVjwG6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#处理语料库\n",
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)\n",
        "\n",
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ],
      "metadata": {
        "id": "vxAlemSlwG6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 拆分训练集、验证集\n",
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio\n",
        "\n",
        "# 首先会检查是否已经存在了划分好的训练集和验证集，如果存在则跳过这个过程\n",
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "# 如果没有，首先读入语料库的行数，并随机打乱行的顺序\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    # 然后对于每个语言，分别打开训练集和验证集文件，并按照指定的比例将每行数据分配到训练集或验证集中去\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "cQfyeq3UwG6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword Units\n",
        "\n",
        "翻譯存在的一大問題是未登錄詞(out of vocabulary)，可以使用 subword units 作為斷詞單位來解決。\n",
        "\n",
        "* 使用 [sentencepiece](#kudo-richardson-2018-sentencepiece) 套件\n",
        "  \n",
        "* 用 unigram 或 byte-pair encoding (BPE)\n",
        "  \n",
        "\n",
        "* 词汇表用来**将文本数据中的单词映射成机器可读的数字序列**，从而方便输入到神经网络中进行计算。\n",
        "  \n",
        "* 在翻译任务中，同时需要为源语言和目标语言建立两个不同的词汇表，这样可以使得源语言和目标语言的单词不发生冲突，从而避免翻译过程中的错误。\n",
        "  \n",
        "* 分词器则用于**将句子拆分成不同的单词或字**，使得机器可以更加细粒度地处理文本数据。\n",
        "  \n",
        "* 在中文等非空格分隔的语言中，分词器尤为重要，因为无法单纯地按照空格对文本进行分词。\n",
        "  \n",
        "* SentencePiece是一种通用的分词器，可以用于任意语言的分词和字级别建模，极大地方便了跨语言机器翻译的处理和训练。\n",
        "\n",
        "分词器，将句子分成词语\n",
        "词将词映射成机器能读的数字"
      ],
      "metadata": {
        "id": "TsLUnqB6wG6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Subword Units ？？？\n",
        "'''\n",
        "Subword Units\n",
        "Out of vocabulary (OOV) has been a major problem in machine translation.\n",
        "This can be alleviated by using subword units.\n",
        "语汇不足(OOV)一直是机器翻译中的主要问题。这可以通过使用子词单位来缓解。\n",
        "\n",
        "We will use the sentencepiece package\n",
        "select 'unigram' or 'byte-pair encoding (BPE)' algorithm\n",
        "我们将使用句子件包装\n",
        "选择'unigram'或'byte-pair encoding (BPE)'算法\n",
        "'''\n",
        "import sentencepiece as spm\n",
        "# 训练SentencePiece模型，以构建字典和分词器，用于后续的机器翻译任务。训练过程主要包括设置训练参数和调用\n",
        "vocab_size = 8000\n",
        "# 首先检查模型是否已经存在，如果存在则跳过训练过程\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "# 如果模型不存在，则调用SentencePieceTrainer.train函数进行训练\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}', # 输入文件\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}', # 输出模型前缀\n",
        "        vocab_size=vocab_size, # 字典大小\n",
        "        character_coverage=1, # 字符覆盖率\n",
        "        model_type='unigram', # 模型类型， 'bpe' 也可\n",
        "        input_sentence_size=1e6, # 输入句子大小\n",
        "        shuffle_input_sentence=True, # 是否随机打乱输入句子和归一化规则\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )\n",
        "\n",
        "# 训练完成，我们就可以使用SentencePiece分词器来对输入文本进行分词，以用于后续的机器翻译任务\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "# 定义字典in_tag，用于指定输入文件的不同标签\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "# 后遍历训练集、验证集和测试集，对于每个语言，分别对其进行编码。\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        # 如果已经存在了输出文件，则跳过编码过程\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "         # 否则，打开输入文件和输出文件，并遍历输入文件中的每一行。\n",
        "        # 对于每一行，首先进行去除末尾空格的操作，然后调用SentencePiece模型的encode函数进行编码。\n",
        "        # 编码后的结果以空格分隔，写入输出文件即可\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)\n",
        "\n",
        "# 最后，经过编码的语料文件将被用于机器翻译模型的训练和评估\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mgunsf4SwG6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5\n"
      ],
      "metadata": {
        "id": "6CgiaSuLwG6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  用 fairseq 將資料轉為 binary\n",
        "将单词变成数字"
      ],
      "metadata": {
        "id": "5f2XjHVwwG6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Binarize the data with fairseq\n",
        "'''\n",
        "Prepare the files in pairs for both the source and target languages.\n",
        "\\ In case a pair is unavailable, generate a pseudo pair to facilitate binarization.\n",
        "为源语言和目标语言准备成对的文件。\n",
        "如果一对不可用，则生成一个伪对以方便二值化。\n",
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "'''\n",
        "# 使用fairseq_cli对上述已编码的语料库进行预处理，以生成可用于机器翻译训练的数据集`\n",
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "# 使用fairseq_cli对上述已编码的语料库进行预处理，以生成可用于机器翻译训练的数据集\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "# 调用了fairseq_cli.preprocess模块将已编码的语料库转换为二进制格式的数据集，以供后续的机器翻译模型训练和评估\n",
        "else:\n",
        "    # --source-lang和--target-lang参数，指定了源语言和目标语言的缩写\n",
        "    # --trainpref、--validpref和--testpref指定了相应语料库的前缀，也就是数据集的输入文件。这些文件应该是已按照标记进行编码的文件\n",
        "    # --destdir指定了输出数据集文件夹路径\n",
        "    # --joined-dictionary参数表示共享源语言和目标语言的字典，以减小数据集大小并减小训练时间\n",
        "    # --workers参数指定了使用的进程数。在预处理语料库时，可以使用多个进程并行处理以提高速度\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ],
      "metadata": {
        "id": "c6lI3Kt8wG6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "超参数的设定"
      ],
      "metadata": {
        "id": "BWCNJ7RlwG6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Hyper parameter 超参数设定-----\n",
        "config = Namespace(\n",
        "    datadir = \"/kaggle/working/DATA/data-bin/ted2020\",\n",
        "    savedir = \"/kaggle/working/rnn\",\n",
        "    source_lang = src_lang,\n",
        "    target_lang = tgt_lang,\n",
        "\n",
        "    # cpu threads when fetching & processing data.\n",
        "    # 指定了在数据获取和处理时使用的CPU线程数\n",
        "    num_workers=0,\n",
        "    # batch size in terms of tokens.\n",
        "    #  max_tokens指定了每个训练批次的最大令牌数量，\n",
        "    max_tokens=8192,\n",
        "    # gradient accumulation increases the effective batchsize.\n",
        "    # 即在每次反向传播时考虑多少个标记，为了提高训练效率，通常使用梯度积累\n",
        "    # 训练两次把梯度累计起来进行一次更新\n",
        "    accum_steps=2,\n",
        "\n",
        "    # the lr s calculated from Noam lr scheduler. 可以修改 the maximum lr by this factor.\n",
        "    # 动态学习率\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "\n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    #梯度裁剪的预值,以便减轻梯度爆炸的问题\n",
        "    clip_norm=1.0,\n",
        "\n",
        "    # maximum epochs for training\n",
        "    max_epoch=16,\n",
        "    start_epoch=1,\n",
        "\n",
        "    # beam size for beam search\n",
        "    # 波束搜索的大小>\n",
        "    beam=5,\n",
        "    # 最长输出长度为 ax + b,\n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    # max_len_a和max_len_b指定了在解码时生成的最大输出长度。这允许在单个模型中支持可变长度的输出\n",
        "    max_len_a=1.2,\n",
        "    max_len_b=10,\n",
        "    # when decoding, post process sentence by removing sentencepiece symbols.\n",
        "    # 指定了解码后的后处理方法，以去除SentencePiece等标记\n",
        "    post_process = \"sentencepiece\",\n",
        "\n",
        "    # checkpoints\n",
        "    # keep_last_epochs指定了应保留的最后几个训练轮数据。\n",
        "    # 这些数据将用于用新数据替换它们以进行重新训练。这有助于避免过拟合和错误方向的训练，反复使用相同数据构造模型\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # 是否从之前的检查点训练模型\n",
        "\n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.548713Z",
          "iopub.status.idle": "2023-05-06T02:16:34.549393Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.549049Z",
          "shell.execute_reply": "2023-05-06T02:16:34.549085Z"
        },
        "trusted": true,
        "id": "8CLEUnOIwG6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. 梯度积累：\n",
        "  \n",
        "  * 概念：指将多个小批次的梯度累加起来，然后一起执行一次反向传播，并更新模型的参数\n",
        "  * 用途：这样可以将小批次的效率优劣通过累加来平衡，提升梯度的准确性和稳定性\n",
        "  * 累积的次数：（即`accum_steps`）越多，每次反向传播计算的梯度越准确，但训练时间也会减慢\n",
        "2. `beam search` 束搜索 束搜索是一种常用的**生成式建模方法**。束搜索的基本思想是在*输入源文本的基础*上，通过一个经过训练的神经网络模型来生成目标文本。生成时通过保留预测概率最高的K个结果，同时基于每个结果继续扩大搜索范围，以提高翻译准确率。这个搜索过程中，保留预测概率最高的K个结果称为“束”\n",
        "  \n",
        "  * 增加`beam size`有助于提高翻译的准确性，但会显著降低翻译过程的速度和效率\n",
        "3. `resume`\n",
        "  \n",
        "  * `resume`参数用于指定是否从之前的检查点继续训练模型。在本例中，resume的值为None，表示不从检查点继续训练。如果将resume设置为之前的检查点文件名，那么训练将从上一次保存的检查点处继续训练\n",
        "  * 由于某些原因（如服务器故障、网络中断等），训练被中断，需要在此基础上继续进行训练\n",
        "  * 对于非常复杂的模型，模型的训练过程需要数天、数周，为了避免从头开始重新训练，通常会将训练拆分为多个阶段，并在每个阶段结束时保存检查点。在这种情况下，如果要对同一模型进行另一项任务的训练，可以在之前的检查点基础上继续训练，从而节省训练时间和计算资源\n",
        "4. `use_wandb`\n",
        "  \n",
        "  * use_wandb参数用于指定是否使用W&B库（即Weights & Biases库）进行实时记录和可视化训练过程和指标\n",
        "  * W&B是一个工具，可以记录和可视化机器学习模型训练过程中的各种指标和信息，帮助用户更好地理解模型的性能和训练过程。使用W&B库可以实时查看模型的训练/验证/测试曲线、参数的分布情况等"
      ],
      "metadata": {
        "id": "tHO2QKIMwG6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Logging套件\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.550814Z",
          "iopub.status.idle": "2023-05-06T02:16:34.551742Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.551479Z",
          "shell.execute_reply": "2023-05-06T02:16:34.551507Z"
        },
        "trusted": true,
        "id": "b9z1qLV_wG6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "讀取資料集\n",
        "\n",
        "## 借用 fairseq 的 TranslationTask\n",
        "\n",
        "* 用來讀進上面 binarized 的檔案\n",
        "* 有現成的 data iterator (dataloader)\n",
        "* 字典 task.source_dictionary 和 task.target_dictionary 也很好用\n",
        "* 有實做 beam search"
      ],
      "metadata": {
        "id": "bo_D22a3wG6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ Dataset & Dataloader ------\n",
        "\n",
        "#Dataloading\n",
        "\n",
        "# 从fairseq设置一个机器翻译任务，返回一个translationTask操作对象\n",
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "# 配置机器翻译任务的参数\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir, #数据路径\n",
        "    source_lang=config.source_lang,# 源语言\n",
        "    target_lang=config.target_lang,# 目标语言\n",
        "    train_subset=\"train\", # 训练集子集\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "# 用于创建一个TranslationTask操作对象，\n",
        "# 其内部会根据上一步的参数配置来构建相应的模型和数据处理流程\n",
        "task = TranslationTask.setup_task(task_cfg)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.55829Z",
          "iopub.status.idle": "2023-05-06T02:16:34.558757Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.558489Z",
          "shell.execute_reply": "2023-05-06T02:16:34.558514Z"
        },
        "trusted": true,
        "id": "FldKwZE9wG6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logger.info(\"loading data for epoch 1\")\n",
        "# 调用TranslationTask对象的load_dataset()方法，从指定数据路径中读取处于训练集中的数据，并经过预处理后存储于缓存中\n",
        "# split=\"train\"表示加载训练集，epoch=1表示读取处于第1个epoch时的训练数据，combine=True表示如果有回译数据可用，则将其与训练数据合并再进行进一步处理\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)\n"
      ],
      "metadata": {
        "id": "uOLdy8ykwG6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从fairseq任务的验证数据集中随机采样一个样本，并输出其源语言和目标语言文本内容\n",
        "# 展示fairseq任务数据集中样本的内容，以检验数据的正确性和准确性`\n",
        "sample = task.dataset(\"valid\")[1] # 从fairseq的Validation数据集中获取索引为1的样本\n",
        "pprint.pprint(sample)\n",
        "# 使用fairseq提供的string()函数将该样本的源语言内容转换为文本形式，并输出\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "# 将目标语言转化为文本并输出\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "U7TLVI1AwG6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Iterator\n",
        "\n",
        "* 將每個 batch 控制在 N 個 token 讓 GPU 記憶體更有效被利用\n",
        "  \n",
        "* 讓 training set 每個 epoch 有不同 shuffling\n",
        "  \n",
        "* 濾掉長度太長的句子\n",
        "  \n",
        "* 將每個 batch 內的句子 pad 成一樣長，好讓 GPU 平行運算\n",
        "  \n",
        "* 加上 eos 並 shift 一格\n",
        "  \n",
        "  * teacher forcing: 為了訓練模型根據prefix生成下個字，decoder的輸入會是輸出目標序列往右shift一格。\n",
        "    \n",
        "  * 一般是會在輸入開頭加個bos token (如下圖)”![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    \n",
        "  * fairseq 則是直接把 eos 挪到 beginning，訓練起來效果其實差不多。例如:\n",
        "    \n",
        "        # 輸出目標 (target) 和 Decoder輸入 (prev_output_tokens):\n",
        "                     eos = 2\n",
        "                  target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "        prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8"
      ],
      "metadata": {
        "id": "FmGpy8AWwG6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 根据fairseq任务和数据集配置参数，创建一个数据迭代器，并使用该迭代器按batch逐一获取训练或验证数据\n",
        "# 主要目的是为了检查数据是否被正确读入，并且查看当前batch的训练数据结构是否满足需求\n",
        "\n",
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens, #一个batch中最多多少个token\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions( # 确保可以处理的序列最大长度（以 tokens 为单位）\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True, # 跳过无效的输入数据(数据格式不正确，长度为0，无效字符之类的)\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond\n",
        "        # first call of this method has no effect.\n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True) # 获取下一个batch的样本\n",
        "sample = next(demo_iter)#获取迭代器中下一个元素，即本次迭代的数据样本。得到的sample对象包括了当前batch内所有样本的信息\n",
        "sample"
      ],
      "metadata": {
        "id": "7ByUFxvQwG6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "函数 load_data_iterator 的参数说明：\n",
        "\n",
        "* `task`: 模型任务。\n",
        "* `split`: 读取哪部分的数据集，可以是 \"train\" 或 \"valid\"。\n",
        "* `epoch`: 当前轮次的训练迭代次数，默认为 1。\n",
        "* `max_tokens`: 每个 batch 中最多包含多少个 token，用于控制内存使用和训练速度。\n",
        "* `num_workers`: 用于数据加载的线程数。\n",
        "* `cached`: 是否缓存迭代器。\n",
        "\n",
        "返回值 `batch_iterator` 是通过 `get_batch_iterator` 方法创建的迭代器，其中包括数据集、batch大小等参数。返回的迭代器对象是 `FairseqIterativeDataset` 类的对象，可以用于获取和处理数据集中的数据。next_epoch_itr(shuffle=True) 方法用于获取下一个 batch 的样本，可以通过 next 函数获取迭代器中下一个元素，即本次迭代的数据样本，这个样本对象包括了当前 batch 内所有样本的信息。\n",
        "\n",
        "* 每個 batch 是一個字典，key 是字串，value 是 Tensor，內容說明如下\n",
        "```python\n",
        "batch = {\n",
        "  \"id\": id, # 每個 example 的 id\n",
        "  \"nsentences\": len(samples), # batch size 句子數\n",
        "  \"ntokens\": ntokens, # batch size 字數\n",
        "  \"net_input\": {\n",
        "      \"src_tokens\": src_tokens, # 來源語言的序列\n",
        "      \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
        "      \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
        "  },\n",
        "  \"target\": target, # 目標序列\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "m575WAaEwG6z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3FxqbMswG6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "定義模型架構\n",
        "\n",
        "* 我們一樣繼承 fairseq 的 encoder, decoder 和 model, 這樣測試階段才能直接用他寫好的 beam search 函式"
      ],
      "metadata": {
        "id": "98226XspwG60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, # fairseq中的Encoder基类，所有Encoder模块都必须继承该类\n",
        "    FairseqIncrementalDecoder, # fairseq中的Incremental Decoder基类，所有Incremental Decoder模块都必须继承该类\n",
        "    FairseqEncoderDecoderModel # Encoder-Decoder模型的基类，是实现sequence-to-sequence任务的核心\n",
        ")"
      ],
      "metadata": {
        "id": "tSSEZc7HwG60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder 編碼器\n",
        "\n",
        "* seq2seq 模型的編碼器為 RNN 或 Transformer Encoder，以下說明以 RNN 為例，Transformer 略有不同。對於每個輸入，Encoder 會輸出一個向量和一個隱藏狀態(hidden state)，並將隱藏狀態用於下一個輸入。換句話說，Encoder 會逐步讀取輸入序列，並在每個 timestep 輸出單個向量，以及在最後 timestep 輸出最終隱藏狀態(content vector)\n",
        "  \n",
        "* 參數:\n",
        "  \n",
        "  * *args*\n",
        "    * encoder_embed_dim 是 embedding 的維度，主要將 one-hot vector 的單詞向量壓縮到指定的維度，主要是為了降維和濃縮資訊的功用\n",
        "    * encoder_ffn_embed_dim 是 RNN 輸出和隱藏狀態的維度(hidden dimension)\n",
        "    * encoder_layers 是 RNN 要疊多少層\n",
        "    * dropout 是決定有多少的機率會將某個節點變為 0，主要是為了防止 overfitting ，一般來說是在訓練時使用，測試時則不使用\n",
        "  * *dictionary*: fairseq 幫我們做好的 dictionary. 在此用來得到 padding index，好用來得到 encoder padding mask.\n",
        "  * *embed_tokens*: 事先做好的詞嵌入 (nn.Embedding)\n",
        "* 輸入:\n",
        "  \n",
        "  * *src_tokens*: 英文的整數序列 e.g. 1, 28, 29, 205, 2\n",
        "* 輸出:\n",
        "  \n",
        "  * *outputs*: 最上層 RNN 每個 timestep 的輸出，後續可以用 Attention 再進行處理\n",
        "  * *final_hiddens*: 每層最終 timestep 的隱藏狀態，將傳遞到 Decoder 進行解碼\n",
        "  * *encoder_padding_mask*: 告訴我們哪些是位置的資訊不重要。\n",
        "\n",
        "### timestep（时间步）\n",
        "\n",
        "* 概述：用于描述一个序列模型处理序列数据的过程，其中每个时间步处理序列的一个元素\n",
        "* 在seq2seq模型的编码器中，timestep指的是编码器在处理输入序列时所处的时刻数，也即是处理序列中的第几个元素\n",
        "* 编码器会对输入序列进行逐个元素处理，每处理一个元素就会输出一个向量和一个隐状态，并将隐状态用于下一个输入元素的处理。\n",
        "* 在每个timestep上，编码器都会处理输入序列的一个元素，一直处理到序列的末尾，最后输出一个最终的隐状态或者是context vector"
      ],
      "metadata": {
        "id": "Z1AFO9B5wG60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Rnn？？\n",
        "class RNNEncoder(FairseqEncoder): #注意这逼fairseq\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        #继承args罢\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_dim = args.encoder_embed_dim\n",
        "        self.hidden_dim = args.encoder_ffn_embed_dim # 每个GRU单元的隐藏层维度\n",
        "        self.num_layers = args.encoder_layers # num_layers是GRU的层数\n",
        "         # 创建一个dropout_in_module实例\n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        #看下面(恼)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim,\n",
        "            self.hidden_dim,\n",
        "            self.num_layers,\n",
        "            dropout=args.dropout,\n",
        "            # 设置batch_first=False（即不使用batch_first模式）\n",
        "            # bidirectional=True，表示该GRU是双向的\n",
        "            batch_first=False,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "\n",
        "        self.padding_idx = dictionary.pad() # 填充字符的索引\n",
        "\n",
        "    # 将双向GRU的hidden state沿着最后一维（即direction维）拼接在一起\n",
        "    def combine_bidir(self, outs, bsz: int):\n",
        "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
        "        return out.view(self.num_layers, bsz, -1)\n",
        "\n",
        "    def forward(self, src_tokens, **unused): # src_tokens是待编码的源序列输入\n",
        "        bsz, seqlen = src_tokens.size() # 获取bsz和seqlen分别为输入src_tokens的batch size和序列长度\n",
        "        # embedding层\n",
        "         # 输入的src_tokens进行embedding并赋值给x\n",
        "        x = self.embed_tokens(src_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "        # B x T x C -> T x B x C 以适应双向GRU的输入格式\n",
        "        x = x.transpose(0, 1)\n",
        "        # 双向RNN\n",
        "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
        "        x, final_hiddens = self.rnn(x, h0)\n",
        "        outputs = self.dropout_out_module(x)\n",
        "        # outputs = [sequence len, batch size, hid dim * directions]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "\n",
        "        # 由于编码器是双向的，我们需要将两个方向的隐藏状态连接起来\n",
        "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
        "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
        "\n",
        "        #这啥啊\n",
        "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
        "        return tuple(\n",
        "            (\n",
        "                outputs,  # seq_len x batch x hidden outputs为全序列的每个位置上模型输出的最后一层RNN隐状态\n",
        "                final_hiddens,  # num_layers x batch x num_directions*hidden final_hiddens为最终的隐藏状态\n",
        "                encoder_padding_mask,  # seq_len x batch 进行padding的输入标记，形状为(T, B)。\n",
        "            )\n",
        "        )\n",
        "    #不需要理这些事（恼）\n",
        "    def reorder_encoder_out(self, encoder_out, new_order):\n",
        "         # 這個beam search時會用到，意義並不是很重要\n",
        "        # new_order是一个张量，代表新的排列顺序。这里我们假设new_order的形状为(batch_size, )\n",
        "        # 根据给定的新顺序new_order，重新排列encoder输出\n",
        "        return tuple(\n",
        "            (\n",
        "                encoder_out[0].index_select(1, new_order),\n",
        "                encoder_out[1].index_select(1, new_order),\n",
        "                encoder_out[2].index_select(1, new_order),\n",
        "            )\n",
        "        )\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.56021Z",
          "iopub.status.idle": "2023-05-06T02:16:34.560816Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.560475Z",
          "shell.execute_reply": "2023-05-06T02:16:34.560502Z"
        },
        "trusted": true,
        "id": "CAMGjMYAwG60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention\n",
        "\n",
        "* 當輸入過長，或是單獨靠 “content vector” 無法取得整個輸入的意思時，用 Attention Mechanism 來提供 Decoder 更多的資訊\n",
        "  \n",
        "* 根據現在 **Decoder embeddings** ，去計算在 **Encoder outputs** 中，那些与其有较高的关系，根据关系的數值來把 Encoder outputs 平均起來作為 **Decoder** RNN 的輸入\n",
        "  \n",
        "* 常見 Attention 的實作是用 Neural Network / Dot Product 來算 **query** (decoder embeddings) 和 **key** (Encoder outputs) 之間的關係，再對所有算出來的數值做 **softmax** 得到分布，最後根据这个分布對 **values** (Encoder outputs) 做 **weight sum**\n",
        "  \n",
        "* 參數:\n",
        "  \n",
        "  * *input_embed_dim*: key 的维度，应该是 decoder 要做 attend 時的向量的维度\n",
        "  * *source_embed_dim*: query 的维度，應是要被 attend 的向量(encoder outputs)的维度\n",
        "  * *output_embed_dim*: value 的维度，應是做完 attention 後，下一層預期的向量维度\n",
        "* 輸入:\n",
        "  \n",
        "  * *inputs*: 就是 key，要 attend 別人的向量\n",
        "  * *encoder_outputs*: 是 query/value，被 attend 的向量\n",
        "  * *encoder_padding_mask*: 告訴我們哪些是位置的資訊不重要。\n",
        "* 輸出:\n",
        "  \n",
        "  * *output*: 做完 attention 后的 context vector（上下文向量），来表示解码器对于编码器输出序列的\"注意力\"。最后，Context Vector将会被拼接到Decoder的输入中，以帮助Decoder生成下一个输出词语。\n",
        "  * *attention score*: attention 的分布\n",
        "\n",
        "### Attention机制\n",
        "\n",
        "1. `Key Vector`：Key Vector是需要被attend的向量，通常是**编码器（Encoder）的输出序列**（也可以是其他任何与当前任务相关的向量）\n",
        "2. `Query Vector`：Query Vector通常是**解码器（Decoder）的当前隐藏状态**（hidden state），即Decoder的当前隐层状态\n",
        "3. `Value Vector`：Value Vector是Key Vector的值，**即在Encoder输出序列中的每个词语的表示向量**。在Seq2Seq模型中，这个向量通常就是Encoder输出的状态。\n",
        "\n",
        "* 在Seq2Seq模型中，Encoder的输出对应Value，而Decoder的隐藏状态对应Query，我们需要计算出Decoder向量与每个Encoder输出之间的关系，并根据这个关系进行加权求和。\n",
        "* `Query Vector`通常与`Key Vector`之间的相似度（或关系）用于计算权重：\n",
        "  * attention_score = softmax(score)\n",
        "  * context_vector = sum(attention_score * Value)\n",
        "* 其中，`attention_score`为一个权重向量，它的和为1，这样我们就得到了加权求和的上下文向量context_vector。在Seq2Seq模型中，这个上下文向量将与Decoder的当前状态进行拼接，作为Decoder的下一个输入\n",
        "* Mask：需要注意的是，在计算Attention Score之前，还需要应用一个掩码（mask）来过滤掉Padding的位置，从而得到正确的attention分布。在Seq2Seq模型中，这个掩码将由Encoder的padding mask传递给Attention。(只看左边)"
      ],
      "metadata": {
        "id": "bZmUyq1TwG61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Attention\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
        "        super().__init__()\n",
        "        #输入输出层先定义好\n",
        "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
        "        self.output_proj = nn.Linear(\n",
        "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
        "        # inputs: T, B, dim\n",
        "        # encoder_outputs: S x B x dim\n",
        "        # padding mask:  S x B\n",
        "\n",
        "        # 先把维度调整好\n",
        "        inputs = inputs.transpose(1,0) # B, T, dim\n",
        "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
        "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
        "\n",
        "        #输入层，先过liner调整维数\n",
        "        x = self.input_proj(inputs)\n",
        "\n",
        "        # compute attention\n",
        "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
        "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
        "\n",
        "        # # 擋住padding位置的attention\n",
        "        if encoder_padding_mask is not None:\n",
        "             # 利用broadcast  B, S -> (B, 1, S)\n",
        "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1) #B, S -> (B, 1, S)\n",
        "            attn_scores = (\n",
        "                attn_scores.float()\n",
        "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
        "                .type_as(attn_scores)\n",
        "            )  # FP16 support: cast to float and back ？？\n",
        "\n",
        "        # 给对应原序列的层来个softmax\n",
        "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
        "        # 批量矩阵乘法，即将两个三维张量中的每个矩阵相乘，得到一个新的三维张量\n",
        "        x = torch.bmm(attn_scores, encoder_outputs)\n",
        "\n",
        "        # (B, T, dim)\n",
        "        x = torch.cat((x, inputs), dim=-1) #拼接\n",
        "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
        "\n",
        "        # restore shape (B, T, dim) -> (T, B, dim)\n",
        "        return x.transpose(1,0), attn_scores\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SUNrm6F0wG61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder 解碼器\n",
        "\n",
        "* 解码器的 `hidden states` 會用编码器最终隐藏状态來初始化(`content vector`)\n",
        "* 解码器同時也根据目前 `timestep` 的輸入(也就是前几个 timestep 的 output)，改变 `hidden states`，並輸出結果\n",
        "* 如果加入 `attention` 可以使表現更好\n",
        "* 我們把 seq2seq 步驟寫在解码器里，好讓等等 Seq2Seq 這個型別可以通用 RNN 和 Transformer，而不用再改寫\n",
        "\n",
        "* 参数:\n",
        "  * *args*\n",
        "    * `decoder_embed_dim` 是解码器 embedding 的維度，类同 encoder_embed_dim，\n",
        "    * `decoder_ffn_embed_dim` 是解码器 RNN 的隐藏维度，類同 encoder_ffn_embed_dim\n",
        "    * `decoder_layers` 解码器 RNN 的层数\n",
        "    * `share_decoder_input_output_embed`通常 decoder 最後輸出的投影矩陣會和輸入 embedding 共用參數\n",
        "  * *dictionary*: fairseq 幫我們做好的 dictionary.\n",
        "  * *embed_tokens*: 事先做好的词嵌入(nn.Embedding)\n",
        "* 輸入:\n",
        "  * *prev_output_tokens*: 英文的整數序列 e.g. 1, 28, 29, 205, 2 已經 shift 一格的 target\n",
        "  * *encoder_out*: 編碼器的輸出\n",
        "  * *incremental_state*: 這是測試階段為了加速，所以會記錄每個 timestep 的 hidden state 詳見 forward\n",
        "* 輸出:\n",
        "  * *outputs*: decoder 每個 timestep 的 logits，還沒經過 softmax 的分布\n",
        "  * *extra*: 沒用到"
      ],
      "metadata": {
        "id": "XOuyVNBmwG61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Decoder 酷诶，但是换成transformer了捏\n",
        "#但是学学（悲）\n",
        "class RNNDecoder(FairseqIncrementalDecoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens): #arg是一个参数\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "\n",
        "        #对args进行一个继承！！\n",
        "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder\n",
        "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
        "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires\n",
        "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
        "\n",
        "        self.embed_dim = args.decoder_embed_dim\n",
        "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
        "        self.num_layers = args.decoder_layers\n",
        "        self.dropout_in_module = nn.Dropout(args.dropout) #抛dropout还要单独标记嘛\n",
        "\n",
        "        self.rnn = nn.GRU( #多层门控循环单元循环神经网络\n",
        "            self.embed_dim, #输入特征数\n",
        "            self.hidden_dim, #隐藏状态特征数量\n",
        "            self.num_layers, #层数\n",
        "            dropout=args.dropout, #丢弃率\n",
        "            batch_first=False, #批次优先no\n",
        "            bidirectional=False #双向NO\n",
        "        )\n",
        "        self.attention = AttentionLayer( #Attention没啥好说\n",
        "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
        "        )\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "\n",
        "        #如果hidden_dim!=embed_dim，那么加一层linear变化\n",
        "        if self.hidden_dim != self.embed_dim:\n",
        "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
        "        else:\n",
        "            self.project_out_dim = None\n",
        "\n",
        "        #如果输入输出embed层相同，那么输出project层直接用相同的embed\n",
        "        if args.share_decoder_input_output_embed:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.embed_tokens.weight.shape[1],\n",
        "                self.embed_tokens.weight.shape[0],\n",
        "                bias=False,\n",
        "            )\n",
        "            self.output_projection.weight = self.embed_tokens.weight\n",
        "        #否则自己开一个dict的embed\n",
        "        else:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.output_embed_dim, len(dictionary), bias=False\n",
        "            )\n",
        "            nn.init.normal_( #正态分布中生成值填充该层参数，实现神经网络参数初始化\n",
        "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
        "            )\n",
        "\n",
        "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
        "        #  从encoder中弄出output\n",
        "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
        "        # outputs:          seq_len x batch x num_directions*hidden\n",
        "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
        "        # padding_mask:     seq_len x batch\n",
        "\n",
        "        if incremental_state is not None and len(incremental_state) > 0:\n",
        "            # 如果上一个时间步的信息被保留，我们可以从那里继续，而不是从bos开始\n",
        "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        else:\n",
        "            # 增量状态不存在，要么是训练时间，要么是测试时间的第一个时间步\n",
        "            # 为seq2seq做准备:将encoder_hidden传递给解码器隐藏状态\n",
        "            prev_hiddens = encoder_hiddens\n",
        "\n",
        "        bsz, seqlen = prev_output_tokens.size()\n",
        "\n",
        "        # embed tokens\n",
        "        x = self.embed_tokens(prev_output_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "        x = x.transpose(0, 1) # B x T x C -> T x B x C\n",
        "\n",
        "        # decoder-to-encoder attention\n",
        "        if self.attention is not None:\n",
        "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
        "\n",
        "        # 通过单向RNN\n",
        "        x, final_hiddens = self.rnn(x, prev_hiddens) #多层门控循环单元循环神经网络\n",
        "        # outputs = [sequence len, batch size, hid dim]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        x = self.dropout_out_module(x)\n",
        "\n",
        "        # 项目的嵌入大小(如果hidden与嵌入大小不同，且share_embedding为True，那我们需要额外来一层线性层\n",
        "        if self.project_out_dim != None:\n",
        "            x = self.project_out_dim(x)\n",
        "\n",
        "        # project to vocab size\n",
        "        # # 投影到vocab size 的分佈\n",
        "        x = self.output_projection(x)\n",
        "\n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(1, 0)\n",
        "\n",
        "        # 如果是增量，则记录当前时间步的隐藏状态，这些状态将在下一个时间步恢复？\n",
        "        #  # 如果是Incremental, 記錄這個timestep的hidden states, 下個timestep讀回來\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": final_hiddens,\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "\n",
        "        return x, None\n",
        "\n",
        "    #这是啥似乎不用理\n",
        "    def reorder_incremental_state(\n",
        "        self,\n",
        "        incremental_state,\n",
        "        new_order,\n",
        "    ):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        return\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "toy6a_X3wG62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2Seq\n",
        "\n",
        "* 由 **Encoder** 和 **Decoder** 組成\n",
        "* 接收輸入並傳給 **Encoder**\n",
        "* 將 **Encoder** 的輸出傳給 **Decoder**\n",
        "* **Decoder** 根據前幾個 timestep 的輸出和 **Encoder** 輸出進行解碼\n",
        "* 當解碼完成後，將 **Decoder** 的輸出傳回\n",
        "\n",
        "### 用 RNN 实现和 用 Transformer 实现的区别\n",
        "\n",
        "1. `RNN`\n",
        "  \n",
        "  * 编码器和解码器通常都采用循环神经网络结构，如LSTM或GRU。\n",
        "  * RNN能够较好地处理序列中的时序信息，但容易在处理长序列时出现梯度消失或爆炸等问题。\n",
        "  * 因此，为了解决这些问题，通常需要通过采用注意力机制（Attention）或卷积神经网络（CNN）等手段来提高模型的性能。\n",
        "2. `Transformer`\n",
        "  \n",
        "  * 不需要采用循环神经网络，而是采用了全新的自注意力机制（Self-Attention）和多头注意力机制（Multi-Head Attention）,从而避免了RNN中的梯度消失和爆炸问题\n",
        "    \n",
        "  * Transformer的自注意力机制允许模型在不考虑时序的情况下处理序列数据，而多头注意力机制可以理解为同时进行多个注意力的训练，进一步提高了模型的表现力和复杂度。\n",
        "    \n",
        "    * 具体而言，Transformer模型包括**编码器和解码器**两个部分，每个部分由多个完全相同的模块组成。模块内部包含子层，其中包含自注意力机制和前向神经网络（Feed-Forward Neural Network），模块之间使用多头注意力机制进行连接。这种设计使得Transformer能够高效地处理序列数据，特别是长序列数据。\n",
        "\n",
        "总的来说，使用Transformer实现Seq2Seq模型相对于使用RNN实现Seq2Seq模型，具有训练速度更快、处理长序列时出现的梯度消失和爆炸问题更少、表现更优秀等优势。"
      ],
      "metadata": {
        "id": "7vlaR3WwwG62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seq2Seq 把encoder和decoder接一起\n",
        "class Seq2Seq(FairseqEncoderDecoderModel): # 继承了基类FairseqEncoderDecoderModel\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens, #原tocken\n",
        "        src_lengths,\n",
        "        prev_output_tokens, #准备输入decoder的token?\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        #就是把encoder和decoder接起来，没了\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens, #输入token\n",
        "            encoder_out=encoder_out, #把encoder输出输入decoder\n",
        "            src_lengths=src_lengths, #长度\n",
        "            return_all_hiddens=return_all_hiddens, # True就会extra吗\n",
        "        )\n",
        "        return logits, extra # 预测的结果logits和一些附加信息extra"
      ],
      "metadata": {
        "id": "tDsGtXxLwG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#模型实例化\n",
        "\n",
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder,\n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" 按照參數設定建置模型 \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # 詞嵌入\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "    # 編碼器與解碼器\n",
        "    # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    # encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    # decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # 序列到序列模型\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "    # 序列到序列模型的初始化模型参数\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear): # 对于 nn.Linear 类型的参数\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02) # 使用正态分布随机初始化权重，并设置mean和std参数。\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_() # 偏置初始化为 0\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_() # 指定填充（padding）的位置在词汇表中的索引\n",
        "        if isinstance(module, MultiheadAttention): # 对注意力机制的 q(key), k(query), v(value) 投影矩阵分别使用正态分布随机初始化\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters(): # 该循环遍历其所有参数\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1) # 使用均匀分布（U(-0.1, 0.1)）随机初始化该参数的值\n",
        "\n",
        "    # 初始化模型\n",
        "    model.apply(init_params)\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.564325Z",
          "iopub.status.idle": "2023-05-06T02:16:34.564845Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.564564Z",
          "shell.execute_reply": "2023-05-06T02:16:34.56459Z"
        },
        "trusted": true,
        "id": "VTbla0_qwG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#架构相关配置\n",
        "\n",
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=256,\n",
        "    encoder_ffn_embed_dim=512,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=256,\n",
        "    decoder_ffn_embed_dim=1024,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3, # 衰减？0.1好过0.3\n",
        ")\n",
        "\n",
        "# 可以魔改的参数？？\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4 # 多头注意力头数（即每个注意力向量拆分出的子向量数）#开8！\n",
        "    args.encoder_normalize_before=True # 进行layer normalization，可以避免梯度消失问题\n",
        "\n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "\n",
        "    args.activation_fn=\"relu\" # 激活函数\n",
        "    args.max_source_positions=1024 # 源语言句子的最大长度\n",
        "    args.max_target_positions=1024\n",
        "\n",
        "    # 補上我們沒有設定的Transformer預設參數\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)\n",
        "\n",
        "# 更新wandb的配置，以便在记录实验期间使用。这样，可以保存并跟踪实验中使用的所有参数和超参数\n",
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))\n",
        "\n",
        "\n",
        "model = build_model(arch_args, task)\n",
        "logger.info(model) # 用于记录信息和调试消息，将模型打印输出到log中，以方便调试和查看模型结构的展示"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.566739Z",
          "iopub.status.idle": "2023-05-06T02:16:34.574168Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.573809Z",
          "shell.execute_reply": "2023-05-06T02:16:34.573844Z"
        },
        "trusted": true,
        "id": "XH2xUyqNwG63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization 最佳化\n",
        "\n",
        "### Loss: Label Smoothing Regularization\n",
        "\n",
        "* 让模型学习输出不集中的分布，防止模型过度自信\n",
        "* 有時候Ground Truth並非唯一答案，所以在算loss時，我們會保留一部分几率給正確答案以外的label\n",
        "* 可以有效防止过拟合 Overfitting\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)\n",
        "\n",
        "### 标签平滑（Label Smoothing）\n",
        "\n",
        "标签平滑的目的是不让模型过度依赖于某些类别，减缓过拟合风险。标签平滑过程对真实的标签进行编码，即对原有的label做如下转化：\n",
        "\n",
        "* 对于正确的标签一部分概率分配给正确的标签；\n",
        "\n",
        "* 对于非正确标签，将概率分配为相同值。\n",
        "\n",
        "其中“一部分概率”由超参数 $\\epsilon$ 控制，因此 $(1-\\epsilon)\\times100%$ 的权重分配给真实的标签。这部分权重被分散到其他标签中。每个标签都得到 $\\epsilon/(n-1)$ 的概率值。例如，当 $n=10$ 时，正确标签有 $0.9$的概率，并将剩余 $0.1$ 的概率均匀分配给其他9个非正确标签，每个标签的值为 $ 0.1/9=0.011$。"
      ],
      "metadata": {
        "id": "2WmZAQyvwG64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#自定义损失函数\n",
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "         # 实现了标签平滑的交叉熵损失函数。标签平滑的目的是避免模型对于训练数据中的异常标签过于敏感\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing # 标签平滑的值，一般取0.1\n",
        "        self.ignore_index = ignore_index # 需要忽略的标签，默认为None；\n",
        "        self.reduce = reduce # 是否对损失进行求和操作，默认为True\n",
        "\n",
        "    def forward(self, lprobs, target):\n",
        "        # lprobs：预测的log softmax概率分布，形状为(batch_size, seq_length, num_classes)\n",
        "        # target：目标标签，形状为(batch_size, seq_length)\n",
        "\n",
        "        # 首先，如果目标标签的维度比预测的概率分布要低一维，则对目标标签的维度进行升维操作\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "\n",
        "        # nll: 负对数似然，one-hot的交叉shang. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        # 保留是其他label的可能性，因此算CP时.把负对数加起来\n",
        "        # 將一部分正確答案的機率分配給其他label 所以當計算cross-entropy時等於把所有label的log prob加起來\n",
        "        # 将预测的log softmax概率分布lprobs在最后一维上求和得到每个标签的log概率和\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index) #还有要masked的地方\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1) #不用mask只能，直接求和\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce: # 对损失进行求和操作\n",
        "            nll_loss = nll_loss.sum() #这要sum?\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "\n",
        "        # 計算cross-entropy時 加入分配給其他label的loss\n",
        "        # 对交叉熵损失和平滑损失进行加权求和，得到最终的损失值。\n",
        "        # 其中，平滑损失的权重为eps_i，交叉熵损失的权重为(1 - smoothing)\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "\n",
        "# 损失函数实例化\n",
        "# 平滑因子=0.1 ， target_dict就忽略？\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.575952Z",
          "iopub.status.idle": "2023-05-06T02:16:34.576854Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.576564Z",
          "shell.execute_reply": "2023-05-06T02:16:34.576598Z"
        },
        "trusted": true,
        "id": "2w7owNjlwG64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#优化器：Adam+学习率计划\n",
        "#通过自定义优化器实现\n",
        "\n",
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    # TODO: Change lr from constant to the equation shown above\n",
        "    lr = (d_model ** (-0.5)) * min(step_num ** (-0.5), step_num * warmup_step ** (-1.5))\n",
        "    return lr\n",
        "\n",
        "class NoamOpt: #自定义优化器\n",
        "    def __init__(self, model_size, factor, warmup, optimizer): #信息全盘继承就完事了\n",
        "        self.optimizer = optimizer #底层优化器\n",
        "        self._step = 0 #初始化步数\n",
        "        self.warmup = warmup #warmup步数\n",
        "        self.factor = factor #设定学习率\n",
        "        self.model_size = model_size #就model_size\n",
        "        self._rate = 0 #初始化总体学习率\n",
        "\n",
        "    @property #申明只读，不可修改(就const呗)\n",
        "\n",
        "    def param_groups(self): #读取底层优化器的参数表\n",
        "        return self.optimizer.param_groups\n",
        "\n",
        "    def multiply_grads(self, c): # 将梯度乘上常数c\n",
        "        # 在计算完损失函数的梯度后，将相关参数的梯度条目用常数标量c缩放。\n",
        "        # 这个常数标量可以用来进行梯度裁剪操作，以避免梯度爆炸的问题\n",
        "        for group in self.param_groups: #扫所有参数表（每对value-target都有个数据表）\n",
        "            for p in group['params']: #读每个参数表中的params\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c) #梯度存在就直接乘c\n",
        "\n",
        "    def step(self): #更新参数与学习率\n",
        "        self._step += 1 #step++\n",
        "        rate = self.rate() #获取新的学习率\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate #设定每个参数表的学习率\n",
        "        self._rate = rate #总体学习率\n",
        "        self.optimizer.step() #底层优化器开冲\n",
        "\n",
        "    def rate(self, step = None): #当前学习率\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.578508Z",
          "iopub.status.idle": "2023-05-06T02:16:34.57944Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.579115Z",
          "shell.execute_reply": "2023-05-06T02:16:34.579148Z"
        },
        "trusted": true,
        "id": "dz6zw55hwG65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#实例化优化器\n",
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim,\n",
        "    factor=config.lr_factor,\n",
        "    warmup=config.lr_warmup,\n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "\n",
        "#学习率计划可视化\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ],
      "metadata": {
        "id": "atfLD4dKwG66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "訓練步驟\n",
        "\n",
        "### 训练\n",
        "\n",
        "#### 混合精度训练\n",
        "\n",
        "1. 概述\n",
        "  * 混合精度训练是一种**加速深度神经网络训练**的技术\n",
        "  * 通过将模型参数使用较低精度的数值类型存储和更新，从而减少存储和计算的开销，提高训练效率。\n",
        "  * 一般来说，使用低精度的数值类型可能会导致精度下降、梯度消失或爆炸等问题，为了解决这些问题，混合精度训练还需要使用梯度缩放、梯度转换等技术。\n",
        "2. 底层\n",
        "  * 在混合精度训练中，通常将模型参数划分为两类：主精度（master precision）和辅助精度（auxiliary precision）。\n",
        "  * 主精度通常是32位浮点数，用于存储和更新模型参数，而辅助精度则是16位或8位浮点数，用于存储梯度和临时变量。在每次更新模型参数前，可以将梯度从辅助精度转换为主精度，计算完后再转换回来。\n",
        "\n",
        "在 PyTorch 中，可以使用 torch.cuda.amp 模块实现自动混合精度训练，简化了混合精度训练的代码实现。"
      ],
      "metadata": {
        "id": "KYyfCw5fwG66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----训练过程------\n",
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    # 读下一个epoch\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    # 梯度积累:更新每个accum_steps samples\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps)\n",
        "\n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # 自动混合精度 automatic mixed precision (amp)\n",
        "\n",
        "    model.train() #训练模式\n",
        "    progress=tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False) #把扫epoch的进度可视化\n",
        "    for samples in progress:\n",
        "        model.zero_grad() #把梯度清零\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        #enumerate()函数用于将一个可遍历的数据对象（如列表、元组或字符串）组合为一个索引序列，\n",
        "        #同时列出数据和数据下标，一般用在for循环当中。\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                torch.cuda.empty_cache() #清空cuda缓存\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device) #sample=sample.to(device)?\n",
        "            target = sample[\"target\"] #读target\n",
        "            sample_size_i = sample[\"ntokens\"] #读出单个size\n",
        "            sample_size += sample_size_i #求sample总size?\n",
        "\n",
        "\n",
        "            with autocast(): # 混合精度训练\n",
        "                net_output = model.forward(**sample[\"net_input\"]) #求输出(概率)\n",
        "                lprobs = F.log_softmax(net_output[0], -1) #softmax，成句子\n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) #除了最后一维其他拉直，\n",
        "\n",
        "                # 求loss之和\n",
        "                accum_loss += loss.item()\n",
        "                # 反向传播求梯度\n",
        "                scaler.scale(loss).backward() #因为梯度没有清零所以累加在这里了！\n",
        "\n",
        "        #将优化器中的梯度缩放因子还原为1\n",
        "        scaler.unscale_(optimizer)\n",
        "        # 除以sample_size，求得平均梯度\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0))\n",
        "        #计算梯度的范数，并将其裁剪到指定的最大范数/梯度裁剪，防止爆炸\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm)\n",
        "\n",
        "        scaler.step(optimizer) #更新模型的权重\n",
        "        scaler.update() #更新参数\n",
        "\n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size #求得单个batch平均loss\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print) #在进度条中显示损失值\n",
        "        if config.use_wandb: #将指标记录\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "\n",
        "    loss_print = np.mean(stats[\"loss\"]) #求得epoch的平均loss\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.581081Z",
          "iopub.status.idle": "2023-05-06T02:16:34.581904Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.58156Z",
          "shell.execute_reply": "2023-05-06T02:16:34.581592Z"
        },
        "trusted": true,
        "id": "-sdG1cEXwG66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 验证与推理\n",
        "\n",
        "# fairseq 的 beam search generator对一个样本进行翻译，并获得翻译结果\n",
        "# 給定模型和輸入序列，用 beam search 生成翻譯結果\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)  # 对输入样本进行翻译，其中运用束搜索，包含了多个翻译假设和对应的分数\n",
        "    srcs = [] # 输入的 token 源语言序列\n",
        "    hyps = []  # 生成的 token 目标语言序列\n",
        "    refs = [] # 目标 token 参考答案\n",
        "    for i in range(len(gen_out)):\n",
        "        # 對於每個 sample, 收集輸入，輸出和參考答，分别解码成字符串，稍後計算 BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()),\n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 代表取出 beam 內分數第一的輸出結果\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()),\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs\n",
        "\n",
        "import shutil\n",
        "import sacrebleu\n",
        "# 实现了对模型进行验证，使用验证集对模型进行评估，并计算 BLEU 分数\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "\n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "\n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "\n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "\n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "\n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.590402Z",
          "iopub.status.idle": "2023-05-06T02:16:34.591342Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.591011Z",
          "shell.execute_reply": "2023-05-06T02:16:34.591044Z"
        },
        "trusted": true,
        "id": "eOcokAjhwG67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 儲存及載入模型參數"
      ],
      "metadata": {
        "id": "PCtKN662wG67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 验证并保存模型\n",
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):\n",
        "    stats = validate(model, task, criterion) #把验证结果拉下来\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # 保存模型地址\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # 要保存的信息\n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "\n",
        "        #保存当前模型\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "\n",
        "        # 保存机器翻译结果\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # 保存验证集上bleu最好的模型\n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "\n",
        "        # 保存的模型太多了就把之前的删掉\n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "\n",
        "    return stats\n",
        "\n",
        "#加载模型\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\" #要加载的模型名\n",
        "    checkpath = Path(config.savedir)/name #全路径\n",
        "    if checkpath.exists(): #如果模型存在就load\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"]) #模型参数\n",
        "        stats = check[\"stats\"] #blue 与 loss\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"] #已经训练好的步数\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\") #找不到就哭呗"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.593138Z",
          "iopub.status.idle": "2023-05-06T02:16:34.594076Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.593734Z",
          "shell.execute_reply": "2023-05-06T02:16:34.593766Z"
        },
        "trusted": true,
        "id": "0fwLAoGCwG68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "fInG4cU1wG68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 模型与损失函数实例化\n",
        "model = model.to(device=device) #模型\n",
        "criterion = criterion.to(device=device) #损失函数（用LSCE啦）"
      ],
      "metadata": {
        "id": "_mjGmaslwG68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 记录训练信息\n",
        "# 当前任务、编码器、解码器、损失函数和优化器的类型，以及模型的参数数量和训练配置信息\n",
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.595537Z",
          "iopub.status.idle": "2023-05-06T02:16:34.596285Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.596038Z",
          "shell.execute_reply": "2023-05-06T02:16:34.596064Z"
        },
        "trusted": true,
        "id": "aV2tE6DewG69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载训练数据迭代器，从指定的start_epoch开始训练\n",
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    #训练\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    #验证\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    #记录一下epoch结束\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))\n",
        "    #去下一个epoch\n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ],
      "metadata": {
        "id": "ASDAhlG0wG69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "这段代码使用了fairseq库中的average_checkpoints.py脚本，用于将多个训练过程中的checkpoint模型进行平均，得到一个新的平均模型，其效果类似于ensemble集成方法。具体解释如下：\n",
        "\n",
        "* --inputs {checkdir}：表示输入的checkpoint模型所在的目录路径。\n",
        "* --num-epoch-checkpoints 5：表示选择最近的5个epoch的checkpoint进行平均处理。\n",
        "* --output {checkdir}/avg_last_5_checkpoint.pt：表示输出平均后的模型的保存路径和文件名。\n",
        "\n",
        "在训练过程中，使用一些技巧来提升模型性能，其中的一种方法就是使用ensemble集成技术，将多个模型的预测结果进行综合，得到更好的预测结果。而这段代码中的平均checkpoint方法能够让我们在训练过程中就使用ensemble技术，将多个模型的权重进行平均，得到一个新的平均模型，具有比单个模型更强的泛化能力和稳定性。"
      ],
      "metadata": {
        "id": "sZ9GPRi4wG69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 把几个checkpoint取平均 ensemble\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ],
      "metadata": {
        "id": "j5YROMglwG69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confirm model weights used to generate submission\n",
        "\n",
        "这段代码中使用了三个checkpoint模型来对模型进行验证：\n",
        "\n",
        "* checkpoint_last.pt：选择最近一个epoch的checkpoint模型进行验证，即使用最新训练的模型。\n",
        "* checkpoint_best.pt：选择在验证集上表现最好的checkpoint模型进行验证，即使用最有优的模型。\n",
        "* avg_last_5_checkpoint.pt：将最近五个epoch的checkpoint模型进行平均，得到平均后的模型进行验证。\n",
        "\n",
        "代码中使用了try_load_checkpoint函数，该函数用于加载指定名称的checkpoint模型，如果成功加载，则返回True，否则返回False。如果加载成功，则会将模型的参数更新为checkpoint中的参数。\n",
        "\n",
        "validate函数用于在给定数据集上对模型进行验证，其中使用了task、criterion等参数，log_to_wandb参数表示是否将结果记录到wandb中。在这里，我们使用上述三个checkpoint模型对模型进行验证，并输出验证结果。"
      ],
      "metadata": {
        "id": "YT2erFE7wG6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "avg = validate(model, task, criterion, log_to_wandb=False)\n",
        "\n",
        "try_load_checkpoint(model, name=\"checkpoint_best.pt\")\n",
        "best = validate(model, task, criterion, log_to_wandb=False)\n",
        "\n",
        "try_load_checkpoint(model, name=\"checkpoint_last.pt\")\n",
        "last = validate(model, task, criterion, log_to_wandb=False)\n",
        "print(\"avg: \",avg['bleu'])\n",
        "print(\"best: \", best['bleu'])\n",
        "print(\"last: \", last['bleu'])\n",
        "None"
      ],
      "metadata": {
        "id": "bYhKYgqawG6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预测"
      ],
      "metadata": {
        "id": "MY2cpukKwG6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):\n",
        "    #加载数据组与数据个体\n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "\n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            # 翻译\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            # 记录翻译结果\n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "\n",
        "    # 按ID把翻译结果排序\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    #把结果写入文件\n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-06T02:16:34.597616Z",
          "iopub.status.idle": "2023-05-06T02:16:34.598373Z",
          "shell.execute_reply.started": "2023-05-06T02:16:34.598122Z",
          "shell.execute_reply": "2023-05-06T02:16:34.598148Z"
        },
        "trusted": true,
        "id": "_vRgzOnGwG6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_prediction(model, task)"
      ],
      "metadata": {
        "id": "HzUtOe1ywG6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raise"
      ],
      "metadata": {
        "id": "akp8jgG-wG6_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}